{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 SMP ETL Workbook\n",
    "[Josh King](https://github.com/kingjml), *CPS/CRD/ECCC*, 2021  \n",
    "[Benoit Montpetit](https://github.com/ecccben), *CPS/CRD/ECCC*, 2024  \n",
    "[Mike Brady](https://github.com/m9brady), *CPS/CRD/ECCC*, 2024\n",
    "\n",
    "\n",
    "To parameterize and evaluate the radar analysis we calibrate an empirical model to derive snow density and specific surface area (SSA) from SnowMicroPen profiles collected at TVC. SMP profiles were collected in a cross pattern at each of the sites, including calibration profiles at snow pits. This workbook and the imported scripts build on [King et al. 2020](https://tc.copernicus.org/articles/14/4323/2020/tc-14-4323-2020.html) to address specific issues encountered with Tundra deployment of the SMP.\n",
    "\n",
    "**Changes and additions from SMP-Sea-Ice**\n",
    "- *ssa*: New tools to calibrate SSA with IceCube reference measurements\n",
    "- *ssa*: Initial coefficients for SSA adapted from [Calonne, Richter, et al. 2020](https://tc.copernicus.org/articles/14/1829/2020/tc-14-1829-2020.html)\n",
    "- *smpfunc*: Scaling of individual layers now uses scipy.interpolation.zoom instead of scipy.signal.resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish path from notebook\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import warnings\n",
    "\n",
    "# Community imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib\n",
    "\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "plt.rcParams[\"axes.labelsize\"] = 22\n",
    "plt.rcParams[\"axes.labelweight\"] = 'bold'\n",
    "plt.rcParams['xtick.labelsize']=16\n",
    "plt.rcParams['ytick.labelsize']=16\n",
    "\n",
    "# ECCC imports\n",
    "import constants\n",
    "import smpfunc\n",
    "import tvcfunc\n",
    "\n",
    "# Init random seed\n",
    "np.random.seed(constants.RANDOM_SEED) \n",
    "\n",
    "# Load reference and meta datasets from Part 1\n",
    "ref_rho = pd.read_json(\"../Data/ref_rho.json\") # Density reference measurements\n",
    "ref_ssa = pd.read_json(\"../Data/ref_ssa.json\") # SSA reference measurements\n",
    "site_meta = pd.read_json(\"../Data/site_meta.json\") # Reconciled SMP site paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMP meta data generation\n",
    "\n",
    "The following loads SMPmeta data collected in situ and its metadata attaches to the site level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_meta_df = pd.DataFrame()\n",
    "for idx, site in site_meta.iterrows():\n",
    "    smp_meta = pd.read_csv(site['smp_meta'][0]).assign(site = site['site'], path = None)\n",
    "    smp_meta.columns  = ['position', 'file', 'probe', 'pen', 'notes', 'site', 'path']\n",
    "    smp_meta.replace(['-99999', -99999], np.nan, inplace = True)\n",
    "    smp_meta['file'] = smp_meta['file'].astype('Int64')\n",
    "    \n",
    "    for idx, profile in smp_meta.iterrows():\n",
    "        smp_file_match = [file for file in site['pnt_files'] if str(profile.file) in file]\n",
    "        if len(smp_file_match)==1:\n",
    "            smp_meta.iloc[idx, smp_meta.columns.get_loc('path')] = smp_file_match[0]\n",
    "            \n",
    "    smp_meta_df = pd.concat([smp_meta_df,smp_meta], ignore_index = True)\n",
    "smp_meta_df.head()\n",
    "\n",
    "smp_meta_df.to_json(\"../Data/smp_meta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate inter-site profiles to find a best match with reference data\n",
    "We collected 2 or more SMP profiles at each snow pit. Rather than assign the profile we thought was closest the following establishes a best match against the reference measurements. First guess values for density and SSA are taken from [King et al. 2020](https://tc.copernicus.org/articles/14/4323/2020/tc-14-4323-2020.html) and [Calonne, Richter, et al. 2020](https://tc.copernicus.org/articles/14/1829/2020/tc-14-1829-2020.html), respectively.\n",
    "\n",
    "#### Find best profile match with density and SSA first guess \n",
    "- Load associated pit SMP profiles, estimate density and SSA from first guess parameters\n",
    "- Scale each SMP density profile with 500 random members\n",
    "- Compare scaling candidates against reference measurements and score\n",
    "- Select best scaling candidate for each profile and report in metadata\n",
    "\n",
    "The loop scores the SMP derived properties at each snow pit against observations. Most of the time the profile we thought was the best candidate is selected but at sites with >4 profiles available, there can a better match where layer structures are drastically different. This is due to hummock and vegetation interactions over sub-meter distances. This can also happen where the reference profile was of poor quality and the neighbouring profile is a better match.\n",
    "\n",
    "This process is brute force and slow. I've provided data intermediaries so this step can be skipped if desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the available calibration files for each site\n",
    "cal_profiles = smp_meta_df.loc[[('SSA' in x )| ('RHO' in x) for x in smp_meta_df['position']]]\n",
    "print(\"Calibration profiles available at {} sites\".format(len(cal_profiles.site.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best profile match for SMP density and SSA calibrations\n",
    "site_meta = pd.DataFrame(); site_result = pd.DataFrame()\n",
    "num_tests = 500\n",
    "for site in cal_profiles.site.unique():\n",
    "    smp_df = cal_profiles.loc[cal_profiles['site'] == site] # SMP profiles at the current site\n",
    "    smp_data = [smpfunc.load_smp_data(smp_file, constants.WINDOW_SIZE) for smp_file in smp_df['path'].tolist()]\n",
    "    smp_density = [smpfunc.calc_density(profile, constants.KING_COEFF, True) for profile in smp_data]\n",
    "    smp_ssa = [smpfunc.calc_ssa(profile, constants.CALONNE_COEFF, True) for profile in smp_data]\n",
    "    \n",
    "    for idx, profile in enumerate(smp_density):\n",
    "        meta, result = smpfunc.eval_scaling(profile, \n",
    "                                            ref_rho.loc[ref_rho.site.str.contains(site)].copy(), \n",
    "                                            layer_h = constants.L_HEIGHT,\n",
    "                                            num_tests = num_tests)\n",
    "        \n",
    "        site_meta = pd.concat([site_meta,meta.assign(site = site, \n",
    "                                                 ref_type = 'density', \n",
    "                                                 smp_file = smp_df.iloc[idx]['file'])], ignore_index=True)\n",
    "        \n",
    "        result['meta_index'] = meta['index'].values[0]\n",
    "        \n",
    "        site_result = pd.concat([site_result,result.assign(site = site, \n",
    "                                                       ref_type = 'density', \n",
    "                                                       smp_file = smp_df.iloc[idx]['file'])], ignore_index=True)\n",
    "    \n",
    "    for idx, profile in enumerate(smp_ssa):\n",
    "        meta, result = smpfunc.eval_scaling(profile, \n",
    "                                            ref_ssa.loc[ref_ssa.site.str.contains(site)].copy(), \n",
    "                                            layer_h = constants.L_HEIGHT,\n",
    "                                            num_tests = num_tests)\n",
    "        \n",
    "        site_meta = pd.concat([site_meta,meta.assign(site = site, \n",
    "                                                 ref_type = 'ssa', \n",
    "                                                 smp_file = smp_df.iloc[idx]['file'])], ignore_index=True)\n",
    "        \n",
    "        result['meta_index'] = meta['index'].values[0]\n",
    "        \n",
    "        site_result = pd.concat([site_result,result.assign(site = site, \n",
    "                                                       ref_type = 'ssa', \n",
    "                                                       smp_file = smp_df.iloc[idx]['file'])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate results for density and SSA\n",
    "site_meta_dens = site_meta.loc[site_meta['ref_type'] == 'density']\n",
    "site_result_dens = site_result.loc[site_result['ref_type'] == 'density']\n",
    "\n",
    "site_meta_ssa = site_meta.loc[site_meta['ref_type'] == 'ssa']\n",
    "site_result_ssa = site_result.loc[site_result['ref_type'] == 'ssa']\n",
    "\n",
    "# Extract the best match for each site\n",
    "first_match_meta_dens = site_meta_dens.sort_values(['rmse'], ascending=[True]).drop_duplicates(['site'])\n",
    "first_match_result_dens = site_result_dens.loc[site_result_dens['smp_file'].isin(first_match_meta_dens['smp_file'].values)]\n",
    "\n",
    "first_match_meta_ssa = site_meta_ssa.sort_values(['rmse'], ascending=[True]).drop_duplicates(['site'])\n",
    "first_match_result_ssa = site_result_ssa.loc[site_result_ssa['smp_file'].isin(first_match_meta_ssa['smp_file'].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New methods to extract SMP derived properties\n",
    "\n",
    "In [King et al. 2020](https://tc.copernicus.org/preprints/tc-2019-305/) we scaled profiles of L and F directly but doing this introduced noise when calibrating regression coefficients and produced interpolation errors at layer boundaries. For TVC we first find the best scaling parameters and then create a translation function between the original and scaled heights. This way we can sample the density cutter measurements against the original SMP profiles without interpolation. \n",
    "\n",
    "An example of the new approach is shown below with 10k member scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example site\n",
    "site = 'RS28'\n",
    "site_df = first_match_meta_dens.loc[first_match_meta_dens.site.str.contains(site)]\n",
    "site_ref = ref_rho.loc[ref_rho.site.str.contains(site)].copy()\n",
    "smp_file = cal_profiles.loc[cal_profiles['file'] == site_df['smp_file'].values[0]]['path'].values[0]\n",
    "smp_data = smpfunc.load_smp_data(smp_file, constants.WINDOW_SIZE)\n",
    "smp_model_dens = smpfunc.calc_density(smp_data, constants.KING_COEFF, True)\n",
    "\n",
    "meta, result = smpfunc.eval_scaling(smp_model_dens, \n",
    "                                    site_ref, \n",
    "                                    num_tests = 10000,\n",
    "                                    layer_h = constants.L_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup table of original height to scaled height\n",
    "orig_dist, scaled_dist = smpfunc.scale_profile(meta['scale_coeff'][0],\n",
    "                                               smp_model_dens['rel_height'].values,\n",
    "                                               smp_model_dens['rel_height'].values,\n",
    "                                               l_resample = constants.L_HEIGHT)\n",
    "\n",
    "# Grab all the scaled heights within the cutter height\n",
    "scaled_cutter_height = [scaled_dist[np.abs(scaled_dist - x)<=constants.CUTTER_SIZE] for x in site_ref['rel_height'].tolist()]\n",
    "\n",
    "# Extract the mean within the scaled cutter bounds\n",
    "scaled_mean = pd.DataFrame()\n",
    "for cutter_idx in np.arange(0, len(scaled_cutter_height)):\n",
    "    scaled_mean = pd.concat([scaled_mean,pd.DataFrame(smp_model_dens.loc[(smp_model_dens['rel_height']>=scaled_cutter_height[cutter_idx].min()) & \\\n",
    "                  (smp_model_dens['rel_height']<=scaled_cutter_height[cutter_idx].max())].mean()).T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.scatter(orig_dist, scaled_dist, color = 'k', s = 5)\n",
    "ax1.plot([0, 500], [0, 500], 'k-', lw=1, alpha = 0.5)\n",
    "ax1.set_xlim(0,500)\n",
    "ax1.set_ylim(0,500)\n",
    "\n",
    "ax1.set_xlabel('Original distance (mm)')\n",
    "ax1.set_ylabel('Scaled distance (mm)')\n",
    "\n",
    "ax2.scatter(scaled_mean['smp_val'], site_ref['ref_val'], color = 'k', label = \"Original\")\n",
    "ax2.scatter(result['mean_samp'], site_ref['ref_val'], color = 'r', label = \"Scaled\")\n",
    "ax2.plot([100, 500], [100, 500], 'k-', lw=1, alpha = 0.5)\n",
    "ax2.set_xlabel('SMP Density ($kg \\\\cdot m^{-3}$)')\n",
    "ax2.set_ylabel('Pit Density ($kg \\\\cdot m^{-3}$)')\n",
    "\n",
    "ax2.set_xlim(100,500)\n",
    "ax2.set_ylim(100,500)\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Figures/Part_2_SMP_ETL_Fig1.png\" height=\"500px\"></center>\n",
    "\n",
    "<center>Figure: Example of original vs scaled distance and calibrated SMP density vs pit density</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract metrics for all SSA calibration profiles\n",
    "\n",
    "Use the best SMP profile match to extract SMP derived metrics where IceCube measurements are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_result_ssa = pd.DataFrame()\n",
    "for idx, row in first_match_meta_ssa.iterrows():\n",
    "    site_ref = ref_ssa.loc[ref_ssa.site.str.contains(row.site)].copy()\n",
    "    smp_file = cal_profiles.loc[cal_profiles['file'] == row['smp_file']]['path'].values[0]\n",
    "    smp_data = smpfunc.load_smp_data(smp_file, constants.WINDOW_SIZE)\n",
    "    smp_model_ssa = smpfunc.calc_ssa(smp_data, constants.CALONNE_COEFF, True)\n",
    "    \n",
    "    meta, result = smpfunc.eval_scaling(smp_model_ssa, \n",
    "                                        site_ref, \n",
    "                                        num_tests = 10000,\n",
    "                                        layer_h = constants.L_HEIGHT)\n",
    "    \n",
    "    # Create a lookup table of origional height to best scaling\n",
    "    dist_lookup = smpfunc.scale_profile(meta['scale_coeff'][0],\n",
    "                                smp_model_ssa['rel_height'].values,\n",
    "                                smp_model_ssa['rel_height'].values,\n",
    "                                50)\n",
    "    \n",
    "    scaled_cutter_height = [dist_lookup[1][np.abs(dist_lookup[0] - x)<=constants.CUTTER_SIZE] for x in site_ref['rel_height'].tolist()]\n",
    "    \n",
    "    scaled_mean = pd.DataFrame()\n",
    "    for cutter_idx in np.arange(0, len(scaled_cutter_height)):\n",
    "        if scaled_cutter_height[cutter_idx].shape[0]>2:\n",
    "            scaled_mean = pd.concat([scaled_mean,pd.DataFrame(smp_model_ssa.loc[(smp_model_ssa['rel_height']>=scaled_cutter_height[cutter_idx].min()) & \\\n",
    "                              (smp_model_ssa['rel_height']<=scaled_cutter_height[cutter_idx].max())].mean()).T], ignore_index=True)\n",
    "        else: # Catch samples that are outside of the profile\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "                scaled_mean = pd.concat([scaled_mean,pd.DataFrame()], ignore_index=True)\n",
    "                \n",
    "    profile_result = pd.concat([scaled_mean, site_ref.rename(columns={'rel_height':'site_rel_height'}).reset_index(drop = True)], axis=1, sort=False)\n",
    "    scaled_result_ssa = pd.concat([scaled_result_ssa,profile_result], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN results and save to json\n",
    "scaled_clean_ssa = scaled_result_ssa.dropna()\n",
    "scaled_clean_ssa.to_json(\"../Data/Scaled_SMP_ssa.json\")\n",
    "\n",
    "# Calculate RMSE for each profile, remove profiles with errors > 3-sigma\n",
    "profile_rmse_ssa = scaled_clean_ssa.groupby('site').apply(lambda x: tvcfunc.calc_rmse(x))\n",
    "result_ssa = scaled_clean_ssa.loc[scaled_clean_ssa['site'].isin(profile_rmse_ssa.loc[profile_rmse_ssa < profile_rmse_ssa.std()*3].index.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract metrics for all density calibration profiles\n",
    "\n",
    "Use the best SMP profile match to extract SMP derived metrics where density cutter measurements are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_result_dens = pd.DataFrame()\n",
    "for idx, row in first_match_meta_dens.iterrows():\n",
    "    site_ref = ref_rho.loc[ref_rho.site.str.contains(row.site)].copy()\n",
    "    smp_file = cal_profiles.loc[cal_profiles['file'] == row['smp_file']]['path'].values[0]\n",
    "    smp_data = smpfunc.load_smp_data(smp_file, constants.WINDOW_SIZE)\n",
    "    smp_model_rho = smpfunc.calc_density(smp_data, constants.KING_COEFF, True)\n",
    "\n",
    "    meta, result = smpfunc.eval_scaling(smp_model_rho, \n",
    "                                        site_ref, \n",
    "                                        num_tests = 10000,\n",
    "                                        layer_h = constants.L_HEIGHT)\n",
    "    \n",
    "    # Create a lookup table of origional height to best scaling\n",
    "    dist_lookup = smpfunc.scale_profile(meta['scale_coeff'][0],\n",
    "                                smp_model_rho['rel_height'].values,\n",
    "                                smp_model_rho['rel_height'].values,\n",
    "                                50)\n",
    "\n",
    "    scaled_cutter_height = [dist_lookup[1][np.abs(dist_lookup[0] - x)<=constants.CUTTER_SIZE] for x in site_ref['rel_height'].tolist()]\n",
    "\n",
    "    scaled_mean = pd.DataFrame()\n",
    "    for cutter_idx in np.arange(0, len(scaled_cutter_height)):\n",
    "        if scaled_cutter_height[cutter_idx].shape[0]>2:\n",
    "            scaled_mean = pd.concat([scaled_mean,pd.DataFrame(smp_model_rho.loc[(smp_model_rho['rel_height']>=scaled_cutter_height[cutter_idx].min()) & \\\n",
    "                              (smp_model_rho['rel_height']<=scaled_cutter_height[cutter_idx].max())].mean()).T], ignore_index=True)\n",
    "        else: # Catch samples that are outside of the profile\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "                scaled_mean = pd.concat([scaled_mean,pd.DataFrame()], ignore_index=True)\n",
    "                \n",
    "    profile_result = pd.concat([scaled_mean, site_ref.rename(columns={'rel_height':'site_rel_height'}).reset_index(drop = True)], axis=1, sort=False)\n",
    "    scaled_result_dens = pd.concat([scaled_result_dens,profile_result], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN results and save to pickle\n",
    "scaled_clean_dens = scaled_result_dens.dropna()\n",
    "scaled_clean_dens.to_json(\"../Data/Scaled_SMP_dens.json\")\n",
    "\n",
    "# Calculate RMSE for each profile, remove profiles with errors > 3-sigma\n",
    "profile_rmse_dens = scaled_clean_dens.groupby('site').apply(lambda x: tvcfunc.calc_rmse(x))\n",
    "result_dens = scaled_clean_dens.loc[scaled_clean_dens['site'].isin(profile_rmse_dens.loc[profile_rmse_dens < profile_rmse_dens.std()*3].index.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model calibration Density\n",
    "\n",
    "Following the same procedures as King et al. 2020 we recalibrate the density model using OLS regression with 10-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=constants.RANDOM_SEED)\n",
    "rmse = []; error = []; r = []; params = None\n",
    "\n",
    "# Split the dataset into 10 roughly equal groups, \n",
    "# train on all but one test group\n",
    "for train_idx, test_idx in k_fold.split(result_dens):\n",
    "    train = result_dens.iloc[train_idx]\n",
    "    test = result_dens.iloc[test_idx]\n",
    "    \n",
    "    model_rho = ols(\"ref_val ~ np.log(force_median) + np.log(force_median) * l\", train).fit()\n",
    "    predict_rho = model_rho.predict(exog=dict(force_median=test['force_median'], l=test['l']))\n",
    "    \n",
    "    rmse = np.append(rmse, np.sqrt(np.mean((predict_rho - test['ref_val'])**2)))\n",
    "    r = np.append(r,np.corrcoef(predict_rho, test['ref_val'])[1][0])\n",
    "    error = np.append(error, predict_rho - test['ref_val'])\n",
    "    \n",
    "    if params is None:\n",
    "        params = model_rho.params.values\n",
    "    else:\n",
    "        params =  np.vstack((params, model_rho.params.values))\n",
    "        \n",
    "model_ols_tvc = [np.round(params[:,0].mean(),2), np.round(params[:,1].mean(),2),\n",
    "              np.round(params[:,3].mean(),2), np.round(params[:,2].mean(),2)]\n",
    "var_coeffs = [np.round(params[:,0].std(),2), np.round(params[:,1].std(),2),\n",
    "              np.round(params[:,3].std(),2), np.round(params[:,2].std(),2)]\n",
    "\n",
    "n_obs = len(result_dens)\n",
    "rmse = rmse.mean()\n",
    "bias = error.mean()\n",
    "r2 = r.mean()**2\n",
    "\n",
    "print(model_ols_tvc)\n",
    "print('N: %i' % n_obs)\n",
    "print('RMSE: %0.1f' % rmse)\n",
    "print('RMSE percent: %0.2f' % np.round(rmse/result_dens.ref_val.mean(),2))\n",
    "print('bias: %0.1f' % bias)\n",
    "print('R$^2$: %0.2f' % r2)\n",
    "\n",
    "filename = '../Data/density_ols_coeffs.npy'\n",
    "np.save(filename, model_ols_tvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[307.76, 53.81, -44.24, -64.8]  \n",
    "N: 646  \n",
    "RMSE: 31.0  $kg\\cdot m^{-3}$  \n",
    "RMSE percent: 0.12  \\%  \n",
    "bias: -0.0  $kg\\cdot m^{-3}$  \n",
    "$R^2$: 0.81  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the new coeffs to estimate density\n",
    "tvc_dens_ols = model_ols_tvc[0]+\\\n",
    "              (model_ols_tvc[1]*np.log(result_dens['force_median']))+ \\\n",
    "              (model_ols_tvc[2]*np.log(result_dens['force_median'])*result_dens['l'])+ \\\n",
    "               model_ols_tvc[3]*result_dens['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_start_a = 100\n",
    "line_end_a = 500\n",
    "point_size = 20\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "f.subplots_adjust(hspace=.25)\n",
    "ax1.set_xlim(line_start_a,line_end_a)\n",
    "ax1.set_ylim(line_start_a,line_end_a)\n",
    "\n",
    "ax1.scatter(result_dens['ref_val'], tvc_dens_ols, \n",
    "            s = point_size, color ='black', zorder = 1000)\n",
    "ax1.plot([line_start_a, line_end_a], [line_start_a, line_end_a], \n",
    "         '-', color = 'gray' ,alpha= 0.8, zorder = 500)\n",
    "ax1.set_xlabel('Density Cutter $\\\\rho_{snow}$ ($kg \\\\cdot m^{-3}$)')\n",
    "ax1.set_ylabel(\"SMP $\\\\rho_{snow}$ ($kg \\\\cdot m^{-3}$)\")\n",
    "\n",
    "ax1.text(350, 115,'N: %i \\nRMSE: %i $kg \\\\cdot m^{-3}$\\nR$^2$: %0.2f'%(n_obs, rmse, r2))\n",
    "\n",
    "hist_bins = np.arange(100, 500, 25)\n",
    "ax2.hist(result_dens['ref_val'], bins = hist_bins, density = True, color = 'dimgrey', label = 'Density\\nCutter', edgecolor='k')\n",
    "ax2.hist(tvc_dens_ols, bins = hist_bins, density = True, color = 'aqua', alpha = 0.5, label = 'SMP', edgecolor='k')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('$\\\\rho_{snow}$ ($kg\\\\cdot m^{-3}$)')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Figures/Figure6.png\" height=\"500px\"></center>\n",
    "\n",
    "<center>Figure 6 of <a href=\"https://doi.org/10.5194/egusphere-2024-651\">Montpetit et al. (2024)</a>: Results of calibrated SMP snow density measurements</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model calibration SSA\n",
    "\n",
    "Here we calibrate the regression model suggested by [Calonne, Richter, et al. 2020](https://tc.copernicus.org/articles/14/1829/2020/tc-14-1829-2020.html) with modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = {'F':'#ADD8E6', 'H':'#0000FF', 'M':'k', 'R':'#FFB6C1', 'N': '#00FF00'}\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, figsize=(15,10))\n",
    "ax1.scatter(np.log(result_ssa['force_median']), np.log(result_ssa['ref_val']), s =4, c= result_ssa['grain_type'].map(colours))\n",
    "ax2.scatter(np.log(result_ssa['l']), np.log(result_ssa['ref_val']), s =4, c= result_ssa['grain_type'].map(colours))\n",
    "ax1.set_ylabel('$ln(SSA_{IC}$) [m$^2$ kg$^{-1}$]')\n",
    "ax1.set_xlabel(r'$ln(\\tilde{F})$ [N]')\n",
    "ax2.set_xlabel(r'$L$ [mm]')\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Precip.',\n",
    "                          markerfacecolor='#00FF00', markersize=12),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Rounded',\n",
    "                          markerfacecolor='#FFB6C1', markersize=12),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Faceted',\n",
    "                          markerfacecolor='#ADD8E6', markersize=12),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Depth hoar',\n",
    "                          markerfacecolor='#0000FF', markersize=12),\n",
    "                  Line2D([0], [0], marker='o', color='w', label='Mixed',\n",
    "                          markerfacecolor='k', markersize=12)]\n",
    "\n",
    "ax2.legend(handles=legend_elements,loc='center', bbox_to_anchor=(1.25, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Figures/Part_2_SMP_ETL_Fig2.png\" height=\"500px\"></center>\n",
    "\n",
    "<center>Figure: Example SMP parameter space color coded by grain type</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log-log model\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=constants.RANDOM_SEED)\n",
    "rmse = []; error = []; r = []; params = None\n",
    "# Split the dataset into 10 roughly equal groups, \n",
    "# train on all but one test group\n",
    "for train_idx, test_idx in k_fold.split(result_ssa):\n",
    "    train = result_ssa.iloc[train_idx]\n",
    "    test = result_ssa.iloc[test_idx]\n",
    "\n",
    "    model_ssa = ols(\"np.log(ref_val) ~ np.log(l) + np.log(force_median)\", train).fit()\n",
    "    predict_ssa = model_ssa.predict(exog=dict(force_median=test['force_median'], l=test['l']))\n",
    "    \n",
    "    rmse = np.append(rmse, np.sqrt(np.mean((np.exp(predict_ssa) - test['ref_val'])**2)))\n",
    "    r = np.append(r,np.corrcoef(np.exp(predict_ssa), test['ref_val'])[1][0])\n",
    "    error = np.append(error, np.exp(predict_ssa) - test['ref_val'])\n",
    "    \n",
    "    if params is None:\n",
    "        params = model_ssa.params.values\n",
    "    else:\n",
    "        params =  np.vstack((params, model_ssa.params.values))\n",
    "        \n",
    "model_ols_tvc = [np.round(params[:,0].mean(),3), np.round(params[:,1].mean(),3),\n",
    "              np.round(params[:,2].mean(), 3)]\n",
    "var_coeffs = [np.round(params[:,0].std(),2), np.round(params[:,1].std(),2),\n",
    "              np.round(params[:,2].std(),2)]\n",
    "\n",
    "n_obs = len(result_ssa)\n",
    "rmse = rmse.mean()\n",
    "bias = error.mean()\n",
    "r2 = r.mean()**2\n",
    "\n",
    "                       \n",
    "print(model_ols_tvc)\n",
    "print('N: %i' % n_obs)\n",
    "print('RMSE: %0.1f' % rmse)\n",
    "print('RMSE percent: %0.3f' % np.round(rmse/result_ssa.ref_val.mean(),2))\n",
    "print('bias: %0.1f' % bias)\n",
    "print('R$^2$: %0.2f' % r2)\n",
    "\n",
    "filename = '../Data/ssa_ols_coeffs.npy'\n",
    "np.save(filename, model_ols_tvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2.371, -0.699, -0.06]  \n",
    "N: 627  \n",
    "RMSE: 5.0  \n",
    "RMSE percent: 0.290  \n",
    "bias: -0.6  \n",
    "R$^2$: 0.69  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvc_ssa_ols = np.exp(model_ols_tvc[0] + (model_ols_tvc[1]*np.log(result_ssa['l'])) + \\\n",
    "                    (model_ols_tvc[2]*np.log(result_ssa['force_median'])))\n",
    "\n",
    "line_start_a = -5\n",
    "line_end_a = 60\n",
    "point_size = 20\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "\n",
    "ax1.scatter(result_ssa['ref_val'], result_ssa['smp_val'], \n",
    "            s = point_size, color ='darkgrey', zorder = 500)\n",
    "\n",
    "ax1.scatter(result_ssa['ref_val'], tvc_ssa_ols, \n",
    "            s = point_size, color ='k', zorder = 1000)\n",
    "\n",
    "\n",
    "ax1.set_xlim(line_start_a,line_end_a)\n",
    "ax1.set_ylim(line_start_a,line_end_a)\n",
    "\n",
    "ax1.plot([line_start_a, line_end_a], [line_start_a, line_end_a], \n",
    "         '-', color = 'gray' ,alpha= 0.8, zorder = 500)\n",
    "\n",
    "ax1.set_xlabel('IceCube SSA ($m^2 \\\\cdot kg^{-1})$')\n",
    "ax1.set_ylabel('SMP SSA ($m^2 \\\\cdot kg^{-1}$)')\n",
    "ax2.set_xlabel('SSA ($m^2 \\\\cdot kg^{-1}$)')\n",
    "\n",
    "\n",
    "hist_bins = np.arange(0, 70, 2)\n",
    "ax2.hist(tvc_ssa_ols, bins = hist_bins, density = True, color = 'k', alpha = 1, label = 'King-TVC', edgecolor='k')\n",
    "ax2.hist(result_ssa['smp_val'], bins = hist_bins, density = True, color = 'darkgrey',alpha = 0.5, label = 'Calonne2020', edgecolor='k')\n",
    "ax2.hist(result_ssa['ref_val'], bins = hist_bins, density = True, color = 'aqua', label = 'IceCube', edgecolor='k', alpha=0.5)\n",
    "ax2.legend()\n",
    "ax2.text(35, 0.045,'King-TVC \\nN: %i \\nRMSE: %0.1f $m^2 \\\\cdot kg^{-1}$\\nR$^2$: %0.2f'%(n_obs, rmse, r2))\n",
    "ax2.text(35, 0.020,'Calonne2020 \\nN: %i \\nRMSE: %0.1f $m^2 \\\\cdot kg^{-1}$\\nR$^2$: %0.2f'%(n_obs, np.sqrt(np.nanmean((result_ssa['ref_val']-result_ssa['smp_val'])**2)), \n",
    "                                                                                           r2_score(result_ssa['ref_val'], result_ssa['smp_val'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Figures/Figure7.png\" height=\"500px\"></center>\n",
    "\n",
    "<center>Figure 7 of <a href=\"https://doi.org/10.5194/egusphere-2024-651\">Montpetit et al. (2024)</a>: Example of original vs scaled distance and calibrated SMP density vs pit density</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-30, 30, 1)\n",
    "plt.hist(result_ssa['ref_val'] - tvc_ssa_ols, \n",
    "         bins = bins, color = 'grey', density = True)\n",
    "plt.xlabel('Error ($m^2 \\\\cdot kg^{-1}$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Figures/Part_2_SMP_ETL_Fig3.png\" height=\"500px\"></center>\n",
    "\n",
    "<center>Figure: Error distribution of the SMP SSA vs the IceCube SSA measurements</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
